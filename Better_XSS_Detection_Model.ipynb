{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMojhyA-5ATs",
        "outputId": "0661a2b2-b620-4ab4-d3d8-09a09135d08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV 1/2; 1/2] START alpha=0.001, hidden_layer_sizes=(100,), max_iter=300........\n",
            "Iteration 1, loss = 0.32531906\n",
            "Validation score: 0.992516\n",
            "Iteration 2, loss = 0.08577751\n",
            "Validation score: 0.980979\n",
            "Iteration 3, loss = 0.03834355\n",
            "Validation score: 0.998441\n",
            "Iteration 4, loss = 0.02649224\n",
            "Validation score: 0.998129\n",
            "Iteration 5, loss = 0.03579210\n",
            "Validation score: 0.998129\n",
            "Iteration 6, loss = 0.02140377\n",
            "Validation score: 0.998441\n",
            "Iteration 7, loss = 0.01718755\n",
            "Validation score: 0.997505\n",
            "Iteration 8, loss = 0.02373639\n",
            "Validation score: 0.989710\n",
            "Iteration 9, loss = 0.03021476\n",
            "Validation score: 0.998129\n",
            "Iteration 10, loss = 0.01482936\n",
            "Validation score: 0.998129\n",
            "Iteration 11, loss = 0.01865769\n",
            "Validation score: 0.998129\n",
            "Iteration 12, loss = 0.02569481\n",
            "Validation score: 0.993140\n",
            "Iteration 13, loss = 0.01455128\n",
            "Validation score: 0.998129\n",
            "Iteration 14, loss = 0.01887197\n",
            "Validation score: 0.997817\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV 1/2; 1/2] END alpha=0.001, hidden_layer_sizes=(100,), max_iter=300;, score=0.995 total time= 1.8min\n",
            "[CV 2/2; 1/2] START alpha=0.001, hidden_layer_sizes=(100,), max_iter=300........\n",
            "Iteration 1, loss = 0.32130955\n",
            "Validation score: 0.978173\n",
            "Iteration 2, loss = 0.07592944\n",
            "Validation score: 0.992516\n",
            "Iteration 3, loss = 0.03918101\n",
            "Validation score: 0.991269\n",
            "Iteration 4, loss = 0.02531383\n",
            "Validation score: 0.994387\n",
            "Iteration 5, loss = 0.02750928\n",
            "Validation score: 0.994699\n",
            "Iteration 6, loss = 0.01859147\n",
            "Validation score: 0.995946\n",
            "Iteration 7, loss = 0.02337813\n",
            "Validation score: 0.995011\n",
            "Iteration 8, loss = 0.01396044\n",
            "Validation score: 0.995011\n",
            "Iteration 9, loss = 0.01447276\n",
            "Validation score: 0.982226\n",
            "Iteration 10, loss = 0.03044314\n",
            "Validation score: 0.994699\n",
            "Iteration 11, loss = 0.02146519\n",
            "Validation score: 0.995011\n",
            "Iteration 12, loss = 0.01141108\n",
            "Validation score: 0.995635\n",
            "Iteration 13, loss = 0.01050960\n",
            "Validation score: 0.995946\n",
            "Iteration 14, loss = 0.01387927\n",
            "Validation score: 0.995635\n",
            "Iteration 15, loss = 0.02696214\n",
            "Validation score: 0.995946\n",
            "Iteration 16, loss = 0.09985886\n",
            "Validation score: 0.514188\n",
            "Iteration 17, loss = 0.57074696\n",
            "Validation score: 0.992516\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV 2/2; 1/2] END alpha=0.001, hidden_layer_sizes=(100,), max_iter=300;, score=0.996 total time= 1.8min\n",
            "[CV 1/2; 2/2] START alpha=0.001, hidden_layer_sizes=(128,), max_iter=300........\n",
            "Iteration 1, loss = 0.30603147\n",
            "Validation score: 0.982226\n",
            "Iteration 2, loss = 0.08037764\n",
            "Validation score: 0.991269\n",
            "Iteration 3, loss = 0.03702658\n",
            "Validation score: 0.946679\n",
            "Iteration 4, loss = 0.03761552\n",
            "Validation score: 0.994699\n",
            "Iteration 5, loss = 0.02745699\n",
            "Validation score: 0.995011\n",
            "Iteration 6, loss = 0.03444944\n",
            "Validation score: 0.978485\n",
            "Iteration 7, loss = 0.02711428\n",
            "Validation score: 0.995635\n",
            "Iteration 8, loss = 0.02438422\n",
            "Validation score: 0.995635\n",
            "Iteration 9, loss = 0.01449449\n",
            "Validation score: 0.995323\n",
            "Iteration 10, loss = 0.01388870\n",
            "Validation score: 0.993452\n",
            "Iteration 11, loss = 0.01872682\n",
            "Validation score: 0.995323\n",
            "Iteration 12, loss = 0.02547413\n",
            "Validation score: 0.992828\n",
            "Iteration 13, loss = 0.02331851\n",
            "Validation score: 0.990957\n",
            "Iteration 14, loss = 0.02359812\n",
            "Validation score: 0.994075\n",
            "Iteration 15, loss = 0.02507345\n",
            "Validation score: 0.995323\n",
            "Iteration 16, loss = 0.02332945\n",
            "Validation score: 0.995635\n",
            "Iteration 17, loss = 0.01257986\n",
            "Validation score: 0.995323\n",
            "Iteration 18, loss = 0.01309700\n",
            "Validation score: 0.995323\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV 1/2; 2/2] END alpha=0.001, hidden_layer_sizes=(128,), max_iter=300;, score=0.996 total time= 4.3min\n",
            "[CV 2/2; 2/2] START alpha=0.001, hidden_layer_sizes=(128,), max_iter=300........\n",
            "Iteration 1, loss = 0.30205024\n",
            "Validation score: 0.986280\n",
            "Iteration 2, loss = 0.06436892\n",
            "Validation score: 0.992516\n",
            "Iteration 3, loss = 0.03955810\n",
            "Validation score: 0.994387\n",
            "Iteration 4, loss = 0.02715139\n",
            "Validation score: 0.995011\n",
            "Iteration 5, loss = 0.02342145\n",
            "Validation score: 0.996258\n",
            "Iteration 6, loss = 0.03366273\n",
            "Validation score: 0.995323\n",
            "Iteration 7, loss = 0.02075595\n",
            "Validation score: 0.989398\n",
            "Iteration 8, loss = 0.03116509\n",
            "Validation score: 0.996258\n",
            "Iteration 9, loss = 0.01917241\n",
            "Validation score: 0.996570\n",
            "Iteration 10, loss = 0.02273399\n",
            "Validation score: 0.997505\n",
            "Iteration 11, loss = 0.01673687\n",
            "Validation score: 0.994387\n",
            "Iteration 12, loss = 0.02026471\n",
            "Validation score: 0.997194\n",
            "Iteration 13, loss = 0.71744210\n",
            "Validation score: 0.991893\n",
            "Iteration 14, loss = 0.02668848\n",
            "Validation score: 0.995323\n",
            "Iteration 15, loss = 0.02329135\n",
            "Validation score: 0.995323\n",
            "Iteration 16, loss = 0.02147728\n",
            "Validation score: 0.996570\n",
            "Iteration 17, loss = 0.01998374\n",
            "Validation score: 0.996570\n",
            "Iteration 18, loss = 0.01868610\n",
            "Validation score: 0.996882\n",
            "Iteration 19, loss = 0.01791756\n",
            "Validation score: 0.997194\n",
            "Iteration 20, loss = 0.01749865\n",
            "Validation score: 0.997194\n",
            "Iteration 21, loss = 0.01606555\n",
            "Validation score: 0.996258\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV 2/2; 2/2] END alpha=0.001, hidden_layer_sizes=(128,), max_iter=300;, score=0.996 total time= 5.2min\n",
            "Iteration 1, loss = 0.17575461\n",
            "Validation score: 0.992515\n",
            "Iteration 2, loss = 0.03163040\n",
            "Validation score: 0.980820\n",
            "Iteration 3, loss = 0.02054023\n",
            "Validation score: 0.996414\n",
            "Iteration 4, loss = 0.02222614\n",
            "Validation score: 0.996258\n",
            "Iteration 5, loss = 0.01557406\n",
            "Validation score: 0.992047\n",
            "Iteration 6, loss = 0.02234779\n",
            "Validation score: 0.995790\n",
            "Iteration 7, loss = 0.01953417\n",
            "Validation score: 0.996881\n",
            "Iteration 8, loss = 0.01702143\n",
            "Validation score: 0.996725\n",
            "Iteration 9, loss = 0.01824974\n",
            "Validation score: 0.990488\n",
            "Iteration 10, loss = 0.01888072\n",
            "Validation score: 0.997505\n",
            "Iteration 11, loss = 0.01122279\n",
            "Validation score: 0.997505\n",
            "Iteration 12, loss = 0.01938175\n",
            "Validation score: 0.994230\n",
            "Iteration 13, loss = 0.01420359\n",
            "Validation score: 0.996881\n",
            "Iteration 14, loss = 0.01540349\n",
            "Validation score: 0.996414\n",
            "Iteration 15, loss = 0.02238851\n",
            "Validation score: 0.996725\n",
            "Iteration 16, loss = 0.01702482\n",
            "Validation score: 0.997349\n",
            "Iteration 17, loss = 0.01505093\n",
            "Validation score: 0.997193\n",
            "Iteration 18, loss = 0.01445726\n",
            "Validation score: 0.996414\n",
            "Iteration 19, loss = 0.02401130\n",
            "Validation score: 0.995478\n",
            "Iteration 20, loss = 0.01314357\n",
            "Validation score: 0.996881\n",
            "Iteration 21, loss = 0.01846337\n",
            "Validation score: 0.990956\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Best parameters: {'alpha': 0.001, 'hidden_layer_sizes': (128,), 'max_iter': 300}\n",
            "Iteration 1, loss = 0.17575461\n",
            "Validation score: 0.992515\n",
            "Iteration 2, loss = 0.03163040\n",
            "Validation score: 0.980820\n",
            "Iteration 3, loss = 0.02054023\n",
            "Validation score: 0.996414\n",
            "Iteration 4, loss = 0.02222614\n",
            "Validation score: 0.996258\n",
            "Iteration 5, loss = 0.01557406\n",
            "Validation score: 0.992047\n",
            "Iteration 6, loss = 0.02234779\n",
            "Validation score: 0.995790\n",
            "Iteration 7, loss = 0.01953417\n",
            "Validation score: 0.996881\n",
            "Iteration 8, loss = 0.01702143\n",
            "Validation score: 0.996725\n",
            "Iteration 9, loss = 0.01824974\n",
            "Validation score: 0.990488\n",
            "Iteration 10, loss = 0.01888072\n",
            "Validation score: 0.997505\n",
            "Iteration 11, loss = 0.01122279\n",
            "Validation score: 0.997505\n",
            "Iteration 12, loss = 0.01938175\n",
            "Validation score: 0.994230\n",
            "Iteration 13, loss = 0.01420359\n",
            "Validation score: 0.996881\n",
            "Iteration 14, loss = 0.01540349\n",
            "Validation score: 0.996414\n",
            "Iteration 15, loss = 0.02238851\n",
            "Validation score: 0.996725\n",
            "Iteration 16, loss = 0.01702482\n",
            "Validation score: 0.997349\n",
            "Iteration 17, loss = 0.01505093\n",
            "Validation score: 0.997193\n",
            "Iteration 18, loss = 0.01445726\n",
            "Validation score: 0.996414\n",
            "Iteration 19, loss = 0.02401130\n",
            "Validation score: 0.995478\n",
            "Iteration 20, loss = 0.01314357\n",
            "Validation score: 0.996881\n",
            "Iteration 21, loss = 0.01846337\n",
            "Validation score: 0.990956\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Accuracy: 99.73%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     13755\n",
            "           1       1.00      1.00      1.00     12819\n",
            "\n",
            "    accuracy                           1.00     26574\n",
            "   macro avg       1.00      1.00      1.00     26574\n",
            "weighted avg       1.00      1.00      1.00     26574\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_with_custom_features.pkl']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier  # Import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "\n",
        "# Function to load and label datasets\n",
        "def load_data(normal_path, xss_path):\n",
        "    with open(normal_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        normal_urls = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    with open(xss_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        xss_urls = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    normal_labels = [0] * len(normal_urls)  # Label 0 for normal\n",
        "    xss_labels = [1] * len(xss_urls)        # Label 1 for XSS\n",
        "\n",
        "    urls = normal_urls + xss_urls\n",
        "    labels = normal_labels + xss_labels\n",
        "\n",
        "    return pd.DataFrame({'url': urls, 'label': labels})\n",
        "\n",
        "# Load the data\n",
        "data = load_data('C:\\\\Users\\\\Omen\\\\Desktop\\\\XSS\\\\Train_NonXSS.txt', 'C:\\\\Users\\\\Omen\\\\Desktop\\\\XSS\\\\Train_XSS.txt')\n",
        "\n",
        "# Sample a small portion of the data for quicker testing (optional)\n",
        "# Uncomment the following line to sample 10% of the data\n",
        "# data = data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Preprocess the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
        "\n",
        "# Additional custom features\n",
        "data['url_length'] = data['url'].apply(len)\n",
        "data['special_char_count'] = data['url'].apply(lambda x: sum(1 for char in x if char in ['<', '>', '\"', '&']))\n",
        "data['keyword_presence'] = data['url'].apply(lambda x: 1 if any(kw in x.lower() for kw in ['script', 'alert', 'img', 'onerror']) else 0)\n",
        "\n",
        "# Combine TF-IDF features with custom features\n",
        "X_custom = np.hstack([X_tfidf, data[['url_length', 'special_char_count', 'keyword_presence']].values])\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_custom, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to oversample the minority class (XSS)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Define and tune the MLPClassifier using GridSearchCV with fewer folds and early stopping\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(100,), (128,)],  # Smaller layers for faster training\n",
        "    'alpha': [0.001],  # Keep alpha fixed to reduce complexity\n",
        "    'max_iter': [300]   # Maximum iterations\n",
        "}\n",
        "\n",
        "mlp_clf = MLPClassifier(random_state=42, early_stopping=True, verbose=True)  # Early stopping added\n",
        "\n",
        "# Perform Grid Search to tune hyperparameters with fewer folds\n",
        "grid_search = GridSearchCV(mlp_clf, param_grid, cv=2, scoring='accuracy', verbose=10)\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Best parameters found from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Train the best model on resampled data\n",
        "best_mlp_clf = grid_search.best_estimator_\n",
        "best_mlp_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test data with a custom decision threshold\n",
        "y_pred_proba = best_mlp_clf.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.4  # Adjust threshold\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model and vectorizer for later use\n",
        "joblib.dump(best_mlp_clf, 'mlpc_xss_model_with_custom_features.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_with_custom_features.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test without featured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V33GET-GDCBV",
        "outputId": "a80ec270-c75c-48ca-eb0c-92e401730c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and vectorizer\n",
            "Total XSS payloads detected: 42186/42760 (98.66%)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Function to process data and return TF-IDF features plus additional features (URL length, special chars, keyword presence)\n",
        "def process_data(urls, vectorizer):\n",
        "    # Use TF-IDF features (vectorizer was trained with max_features=5000)\n",
        "    X_test_real_tfidf = vectorizer.transform(urls).toarray()\n",
        "    \n",
        "    # Calculate additional features\n",
        "    url_length = np.array([len(url) for url in urls])\n",
        "    special_char_count = np.array([sum(1 for char in url if char in ['<', '>', '\"', '&']) for url in urls])\n",
        "    keyword_presence = np.array([1 if any(kw in url.lower() for kw in ['script', 'alert', 'img', 'onerror']) else 0 for url in urls])\n",
        "\n",
        "    # Combine TF-IDF features with the additional features\n",
        "    X_test_real_features = np.hstack([X_test_real_tfidf, \n",
        "                                      url_length.reshape(-1, 1), \n",
        "                                      special_char_count.reshape(-1, 1), \n",
        "                                      keyword_presence.reshape(-1, 1)])\n",
        "    \n",
        "    return X_test_real_features\n",
        "\n",
        "# Test Model Function to count detected XSS payloads and show the percentage\n",
        "def test_model(test_data_path, vectorizer_path='tfidf_vectorizer_with_custom_features.pkl', model_path='mlpc_xss_model_with_custom_features.pkl', threshold=0.95):\n",
        "    print(\"Loading model and vectorizer\")\n",
        "    \n",
        "    # Load the pre-trained MLPClassifier model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    # Load the vectorizer\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "    # Read the test data\n",
        "    with open(test_data_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        urls = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    # Process the data (use TF-IDF features and additional features)\n",
        "    X_test_real_features = process_data(urls, vectorizer)\n",
        "    \n",
        "    # Predict the probabilities using the MLPClassifier model\n",
        "    y_pred_proba = model.predict_proba(X_test_real_features)[:, 1]\n",
        "    \n",
        "    # Apply the threshold to the predicted probabilities\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    total_xss_detected = np.sum(y_pred)\n",
        "    total_payloads = len(y_pred)\n",
        "\n",
        "    # Calculate the percentage of XSS payloads detected\n",
        "    percentage_detected = (total_xss_detected / total_payloads) * 100 if total_payloads > 0 else 0\n",
        "\n",
        "    # Print the number and percentage of XSS detected\n",
        "    print(f\"Total XSS payloads detected: {total_xss_detected}/{total_payloads} ({percentage_detected:.2f}%)\")\n",
        "    \n",
        "    return y_pred\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the test file (all URLs in this file are XSS)\n",
        "    test_data_path = r'C:\\Users\\Omen\\Desktop\\XSS\\xxs_payloads_04.txt'\n",
        "    \n",
        "    # Call the function to count and print the number and percentage of detected XSS\n",
        "    y_pred = test_model(\n",
        "        test_data_path=test_data_path,  \n",
        "        vectorizer_path='tfidf_vectorizer_with_custom_features.pkl',     \n",
        "        model_path='mlpc_xss_model_with_custom_features.pkl', \n",
        "        threshold=0.95  # Adjust the threshold as needed\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test with prefetch with multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and vectorizer\n",
            "Total XSS payloads detected: 98700/100000 (98.70%)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import joblib\n",
        "\n",
        "# Function to calculate additional features\n",
        "def calculate_features(urls):\n",
        "    url_length = np.array([len(url) for url in urls])\n",
        "    special_char_count = np.array([sum(1 for char in url if char in ['<', '>', '\"', '&']) for url in urls])\n",
        "    keyword_presence = np.array([1 if any(kw in url.lower() for kw in ['script', 'alert', 'img', 'onerror']) else 0 for url in urls])\n",
        "    return url_length, special_char_count, keyword_presence\n",
        "\n",
        "# Function to process data and return TF-IDF features plus additional features\n",
        "def process_data(urls, vectorizer):\n",
        "    # Use TF-IDF features with max_features=5000 to ensure it matches the model\n",
        "    X_test_real_tfidf = vectorizer.transform(urls).toarray()\n",
        "    \n",
        "    # Calculate additional features\n",
        "    url_length, special_char_count, keyword_presence = calculate_features(urls)\n",
        "\n",
        "    # Combine TF-IDF features with the additional features (to total 5003)\n",
        "    X_test_real_features = np.hstack([X_test_real_tfidf, \n",
        "                                      url_length.reshape(-1, 1), \n",
        "                                      special_char_count.reshape(-1, 1), \n",
        "                                      keyword_presence.reshape(-1, 1)])  # 3 custom features\n",
        "    \n",
        "    return X_test_real_features\n",
        "\n",
        "# Worker function for processing batches of data in parallel with thresholding\n",
        "def worker_process(url_chunk, vectorizer, model, threshold=0.5):\n",
        "    try:\n",
        "        # Process the chunk of URLs and get predictions\n",
        "        X_test_real_features = process_data(url_chunk, vectorizer)\n",
        "        # Get probability predictions from the model\n",
        "        y_pred_proba = model.predict_proba(X_test_real_features)[:, 1]\n",
        "        # Apply threshold\n",
        "        predictions = (y_pred_proba >= threshold).astype(int)\n",
        "        return np.sum(predictions), len(predictions)  # Return number of XSS detected and total\n",
        "    except Exception as e:\n",
        "        print(f\"Error in worker process: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "# Test Model Function with multithreading, batch processing, and threshold\n",
        "def test_model(test_data_path, vectorizer_path='tfidf_vectorizer.pkl', model_path='mlpc_xss_model_with_custom_features.pkl', num_threads=16, chunk_size=10000, threshold=0.95):\n",
        "    print(\"Loading model and vectorizer\")\n",
        "    \n",
        "    # Load the pre-trained model\n",
        "    model = joblib.load(model_path)  # Assuming you're using a scikit-learn MLP model\n",
        "    \n",
        "    # Load the vectorizer\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "    # Initialize counts for detected XSS and total payloads\n",
        "    total_xss_detected = 0\n",
        "    total_payloads = 0\n",
        "\n",
        "    futures = []\n",
        "\n",
        "    # Open the test data file\n",
        "    with open(test_data_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        urls = [line.strip() for line in file if line.strip()]\n",
        "    \n",
        "    # Use multithreading to process the data in parallel\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        for i in range(0, len(urls), chunk_size):\n",
        "            batch_chunk = urls[i:i + chunk_size]\n",
        "            # Submit the batch chunk for processing\n",
        "            futures.append(executor.submit(worker_process, batch_chunk, vectorizer, model, threshold))\n",
        "        \n",
        "        # Collect results as they complete\n",
        "        for future in as_completed(futures):\n",
        "            xss_detected, payloads = future.result()\n",
        "            total_xss_detected += xss_detected\n",
        "            total_payloads += payloads\n",
        "\n",
        "    # Calculate the final percentage of XSS detected\n",
        "    if total_payloads > 0:\n",
        "        percentage_detected = (total_xss_detected / total_payloads) * 100\n",
        "    else:\n",
        "        percentage_detected = 0\n",
        "    \n",
        "    # Print the total number and percentage of XSS detected\n",
        "    print(f\"Total XSS payloads detected: {total_xss_detected}/{total_payloads} ({percentage_detected:.2f}%)\")\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the test file (all URLs in this file are XSS or normal)\n",
        "    test_data_path = r'C:\\Users\\Omen\\Desktop\\XSS\\Test_Dataset\\For Speed\\XSS100k.txt'\n",
        "    \n",
        "    # Call the function to count and print the number and percentage of detected XSS\n",
        "    test_model(\n",
        "        test_data_path=test_data_path,  \n",
        "        vectorizer_path='tfidf_vectorizer_with_custom_features.pkl',     \n",
        "        model_path='mlpc_xss_model_with_custom_features.pkl', \n",
        "        num_threads=16,  # Number of threads for parallel processing\n",
        "        chunk_size=10000,  # Process in chunks to manage memory\n",
        "        threshold=0.95  # Set your threshold here\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
